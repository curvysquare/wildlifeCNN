{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQmwgb9qD7gc",
        "outputId": "6a3b3321-5b13-412c-ed95-93b6ea502d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "sanity check 43781 14\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os \n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#ignore cameras 10 and 11\n",
        "\n",
        "def read_spreadsheets():\n",
        "\n",
        "  cam_list = []\n",
        "  cam_list_csv = []\n",
        "  base = 'Cam_'\n",
        "\n",
        "  print(cam_list)\n",
        "\n",
        "  for i in range(1, 17):\n",
        "    cam_list.append(base+str(i))\n",
        "  # print(cam_list)\n",
        "\n",
        "  # cam_list.remove('Cam_10')\n",
        "\n",
        "  spreadsheet_dict = {}\n",
        "\n",
        "  for cam in cam_list:\n",
        "    dir = os.path.join('/', 'Users', 'rhyscooper', 'Desktop', 'wildlifeCNN', 'animalCNN', f'{cam}.csv')\n",
        "    spreadsheet_dict[cam] = pd.read_csv(dir)\n",
        "  del spreadsheet_dict['Cam_10']\n",
        "  del spreadsheet_dict['Cam_11']\n",
        "  return spreadsheet_dict\n",
        "\n",
        "spreadsheet_dict = read_spreadsheets()\n",
        "\n",
        "\n",
        "def amend_cam_16_column(spreadsheet_dict):\n",
        "  c16 = spreadsheet_dict['Cam_16']\n",
        "  c16 = c16.iloc[:, 1:]\n",
        "\n",
        "  c16.insert(0, 'Camera no', 'Cam 16')\n",
        "  spreadsheet_dict['Cam_16'] = c16\n",
        "\n",
        "amend_cam_16_column(spreadsheet_dict)\n",
        "\n",
        "\n",
        "def sanity_check():\n",
        "\n",
        "  sanity_check = [800, 3406, 2540, 426, 4946, 3909, 4076, 3990, 2517, 2262, 2205, 444, 5636, 6624]\n",
        "  sanity_check_sum = sum(sanity_check)\n",
        "  print(\"sanity check\", sanity_check_sum,  len(sanity_check))\n",
        "print(sanity_check())\n",
        "def build_df(spreadsheet_dict):\n",
        "\n",
        "\n",
        "  complete_df = pd.concat(spreadsheet_dict.values(), axis=0)\n",
        "  complete_df.reset_index(drop=True, inplace=True)\n",
        "  return complete_df\n",
        "\n",
        "df = build_df(spreadsheet_dict)\n",
        "# # print(df)\n",
        "# # print(df.columns)\n",
        "\n",
        "# def clean_species_values(df):\n",
        "#   df['Species '] = df['Species '].str.replace(' ', '')\n",
        "#   df.loc[df['Camera no'] == 'Cam 6', 'Species '] = 'cow'\n",
        "#   return df\n",
        "\n",
        "# def get_unique_categorical_values(df, column_name):\n",
        "#     if column_name in df.columns:\n",
        "#         unique_values = df[column_name].unique()\n",
        "#         return unique_values\n",
        "#     else:\n",
        "#         return []\n",
        "\n",
        "# key_mapping = { np.nan:0, 'R':1, 'N':2, 'F':3, 'S':4, 'Owl':5, 'M':6, 'P':7, 'O':8, 'H':9, 'Bi':10, 'Bu':11, 'Jay':12 ,'W':13, 'Dog':14 ,'C':15,\n",
        "#  'B':16, 'cow': 17}\n",
        "# def classification_map(df, keymap):\n",
        "#   df['Species '] = df['Species '].replace(keymap)\n",
        "#   return df\n",
        "\n",
        "\n",
        "# df = clean_species_values(df)\n",
        "\n",
        "\n",
        "# df = classification_map(df, key_mapping)\n",
        "# # print(df)\n",
        "\n",
        "# species_list = get_unique_categorical_values(df= df, column_name='Species ')\n",
        "# print(species_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "OpQJXg9kM4Oo",
        "outputId": "14c0ab6c-116a-4ada-bf73-dc808a277cf9"
      },
      "outputs": [],
      "source": [
        "# df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Record():\n",
        "    def __init__(self, dataframe):\n",
        "        self.df = dataframe\n",
        "        self.dir =  os.path.join('/', 'Users', 'rhyscooper', 'Desktop', 'wildlifeCNN', 'images',)\n",
        "        self.num_rows, self.num_columns = self.df.shape\n",
        "        \n",
        "    def image_loader(self, index): \n",
        "        index_row = np.array(self.df.iloc[index]) \n",
        "        # print(index_row)\n",
        "        filename = index_row[2]\n",
        "        path = os.path.join (self.dir,filename)\n",
        "        img = Image.open(path)\n",
        "        species = index_row[5]\n",
        "        # if show:\n",
        "        #     plt.imshow(img)\n",
        "        #     plt.axis('off')  # Turn off axis labels and ticks\n",
        "        #     plt.show()\n",
        "        # if path:\n",
        "        #     return path, species\n",
        "        # if not path:\n",
        "        return img, species \n",
        "    \n",
        "    def train_set_list(self, N_train_samples):\n",
        "        self.N_train_samples = N_train_samples\n",
        "        self.train_dir_list = []\n",
        "        for i in range(0, N_train_samples):\n",
        "            self.train_dir_list.append((self.image_loader(i)))\n",
        "            \n",
        "    def test_set_list(self):\n",
        "        self.test_dir_list = []\n",
        "        for i in range(self.N_train_samples, self.num_rows) :\n",
        "            self.test_dir_list.append((self.image_loader(i)))        \n",
        "            \n",
        "    def test_set_list_override(self, N_train_samples_override):\n",
        "        self.N_train_samples_overide = N_train_samples_override\n",
        "        self.test_dir_list_override = []\n",
        "        for i in range(0, N_train_samples_override):\n",
        "            self.test_dir_list_override.append((self.image_loader(i)))\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rcd = Record(dataframe = df)\n",
        "\n",
        "# rcd.train_set_list(50)\n",
        "# rcd.test_set_list_override(50)\n",
        "# # rcd.test_set_list()\n",
        "\n",
        "# print(len(rcd.train_dir_list))\n",
        "# print(rcd.train_dir_list)\n",
        "# # print(len(rcd.test_dir_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data.dataloader import Dataset\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CreateDataset():\n",
        "    def __init__(self, df, train, N_train_samples):\n",
        "        self.key_map = { np.nan:0, 'R':1, 'N':2, 'F':3, 'S':4, 'Owl':5, 'M':6, 'P':7, 'O':8, 'H':9, 'Bi':10, 'Bu':11, 'Jay':12 ,'W':13, 'Dog':14 ,'C':15,'B':16, 'cow': 17}\n",
        "        self.cleaned_df = self.clean_species_values(df)\n",
        "        self.category_values = self.get_unique_categorical_values(self.cleaned_df)\n",
        "        self.N_category_values = len(self.category_values)\n",
        "        self.classification_mapped_df = self.classification_map(self.cleaned_df)\n",
        "        self.dataset_df = self.classification_mapped_df\n",
        "        \n",
        "        \n",
        "        self.dir =  os.path.join('/', 'Users', 'rhyscooper', 'Desktop', 'wildlifeCNN', 'images',)\n",
        "        self.num_rows, self.num_columns = self.dataset_df.shape\n",
        "        \n",
        "        self.train = train\n",
        "        self.N_train_samples = N_train_samples\n",
        "        \n",
        "        self.dataset = self.dataset_list_compiler(self.train)\n",
        "    \n",
        "    def clean_species_values(self, df):\n",
        "        df['Species '] = df['Species '].str.replace(' ', '')\n",
        "        # df.loc[df['Camera no'] == 'Cam 6', 'Species '] = 'cow'\n",
        "        return df\n",
        "\n",
        "    def get_unique_categorical_values(self, df, column_name = \"Species \"):\n",
        "        if column_name in df.columns:\n",
        "            unique_values = df[column_name].unique()\n",
        "            return unique_values\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def classification_map(self, df):\n",
        "        df['Species '] = df['Species '].replace(self.key_map)\n",
        "        return df\n",
        "\n",
        "    def image_and_label_loader(self, index): \n",
        "        index_row = np.array(self.dataset_df.iloc[index]) \n",
        "        # print(index_row)\n",
        "        filename = index_row[2]\n",
        "        path = os.path.join (self.dir,filename)\n",
        "        img = Image.open(path)\n",
        "        species = index_row[5]\n",
        "        return img, species \n",
        "        \n",
        "    def dataset_list_compiler(self, train):\n",
        "        image_label_list = []\n",
        "        if train:\n",
        "            for i in range(0, self.N_train_samples):\n",
        "                image_label_list.append((self.image_and_label_loader(i)))        \n",
        "        if not train:\n",
        "            image_label_list = []\n",
        "            for i in range(self.N_train_samples, self.num_rows) :\n",
        "                image_label_list.append((self.image_and_label_loader(i)))                \n",
        "\n",
        "        return image_label_list "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDataset2(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Required class for formatting a dataset and allowing the pytorch DataLoader to retreive images\n",
        "    in the correct form. Takes as input a dataset of PIL images and a transform to allow the images \n",
        "    to be of the right format to be used in subsequent models. Pairs the images with their masks and\n",
        "    turns them into tensors with shape [3, height, width].\n",
        "    '''\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.transform = A.Compose([A.Resize(520, 520, p=1),ToTensorV2()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.dataset[idx][0]\n",
        "        label = self.dataset[idx][1]\n",
        "        image_np = np.array(img)\n",
        "        aug = self.transform(image=image_np)\n",
        "        img=aug['image']\n",
        "\n",
        "        return img, label\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Camera no    No             Filename        Date     Time Species   \\\n",
            "0         Cam 1     1   Cam 1/IMG_0001.JPG         NaN      NaN      NaN   \n",
            "1         Cam 1     2   Cam 1/IMG_0002.JPG         NaN      NaN      NaN   \n",
            "2         Cam 1     3   Cam 1/IMG_0003.JPG  12/23/2022  12:49PM      NaN   \n",
            "3         Cam 1     4   Cam 1/IMG_0004.JPG  12/23/2022  12:49PM      NaN   \n",
            "4         Cam 1     5   Cam 1/IMG_0005.JPG  12/23/2022  12:49PM      NaN   \n",
            "...         ...   ...                  ...         ...      ...      ...   \n",
            "43776    Cam 16  6620  Cam 16/IMG_6620.JPG  01/23/2023  10:15AM      NaN   \n",
            "43777    Cam 16  6621  Cam 16/IMG_6621.JPG  01/23/2023  10:15AM      NaN   \n",
            "43778    Cam 16  6622  Cam 16/IMG_6622.JPG  01/23/2023  10:15AM      NaN   \n",
            "43779    Cam 16  6623  Cam 16/IMG_6623.JPG  01/23/2023  10:15AM      NaN   \n",
            "43780    Cam 16  6624  Cam 16/IMG_6624.JPG  01/23/2023  10:15AM      NaN   \n",
            "\n",
            "       Distance 1  Angle 1  Distance 2  Angle 2  Distance 3  Angle 3  \n",
            "0             NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "1             NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "2             NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "3             NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "4             NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "...           ...      ...         ...      ...         ...      ...  \n",
            "43776         NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "43777         NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "43778         NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "43779         NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "43780         NaN      NaN         NaN      NaN         NaN      NaN  \n",
            "\n",
            "[43781 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)\n",
        "train_dataset = CreateDataset(df, True, 30)\n",
        "train_set = train_dataset.dataset \n",
        "\n",
        "output_shape = train_dataset.N_category_values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math \n",
        "import numpy as np\n",
        "import numpy.random as random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data.dataloader import Dataset\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchinfo import summary\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from PIL import Image\n",
        "import gc\n",
        "import torchmetrics\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "get_unique_categorical_values() missing 1 required positional argument: 'self'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/jg/m6l2g86n6vl9c_nbqcftb2hh0000gn/T/ipykernel_769/4000240282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unique_categorical_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Species '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_unique_categorical_values() missing 1 required positional argument: 'self'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model():\n",
        "\n",
        "    def __init__(self, model_name, optim, loss_type, output_shape=18, verbose=False):\n",
        "        # Initialise class parameters\n",
        "        self.model_name = model_name\n",
        "        self.optimiser_type = optim\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        self.model_dic = {\n",
        "        'DeepLabV3': [torchvision.models.segmentation.DeepLabV3_ResNet101_Weights.DEFAULT,\n",
        "                        torchvision.models.segmentation.deeplabv3_resnet101],\n",
        "    \n",
        "        'FCN' :      [torchvision.models.segmentation.FCN_ResNet101_Weights.DEFAULT,\n",
        "                        torchvision.models.segmentation.fcn_resnet101],\n",
        "    \n",
        "        'LRASPP' :   [torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights.DEFAULT, \n",
        "                        torchvision.models.segmentation.lraspp_mobilenet_v3_large],\n",
        "        }\n",
        "\n",
        "        # Call the createModel function to return the specified model, and initialise accuracy metrics\n",
        "        self.createModel(output_shape, verbose)\n",
        "\n",
        "        \n",
        "    def createModel(self, output_shape, verbose):\n",
        "        \"\"\"Pretrained semantic segmentation model with custom head\n",
        "        Args:\n",
        "            output_shape (int, optional): The number of output channels\n",
        "            in your dataset masks. Defaults to 32.\n",
        "\n",
        "            verbose (bool, optional): Print out the model architecture. \n",
        "            Default is False.\n",
        "\n",
        "        Returns:\n",
        "            model: Returns the desired model with either the ResNet101 (for DeepLabV3 and FCN), \n",
        "            or MobileNet (for LRASPP) backbone.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.weights = self.model_dic[self.model_name][0]\n",
        "        self.model = self.model_dic[self.model_name][1](weights=self.weights)\n",
        "        self.auto_transform = self.weights.transforms()\n",
        "        \n",
        "        # Freeze pretrained \"backbone\" layers\n",
        "        if self.model_name == 'LRASPP':\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if \"backbone\" in name:\n",
        "                    if \"14\" in name or \"15\" in name or \"16\" in name:\n",
        "                        pass     \n",
        "                    else:\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        if self.model_name == 'DeepLabV3' or self.model_name == 'FCN':\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if \"backbone\" in name:\n",
        "                    if \"layer3\" in name or \"layer4\" in name:\n",
        "                        pass     \n",
        "                    else:\n",
        "                        param.requires_grad = False        \n",
        "\n",
        "\n",
        "        # Replace the last classifier layer with a Conv2d layer with the correct output shape\n",
        "        # If the model has an auxiliary classifier, replace the last classifier layer with 32 output channels\n",
        "        \n",
        "        if self.model_name == 'DeepLabV3':\n",
        "            self.model.classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "            try:\n",
        "                self.model.aux_classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "                self.model.aux_classifier.add_module('softmax', nn.Softmax(dim=1))\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        if self.model_name == 'FCN':\n",
        "            self.model.classifier[-1] = nn.Conv2d(512, output_shape, kernel_size=1, stride=1)\n",
        "            try:\n",
        "                self.model.aux_classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "                self.model.aux_classifier.add_module('softmax', nn.Softmax(dim=1))\n",
        "            except:\n",
        "                pass  \n",
        "\n",
        "        if self.model_name == 'LRASPP':\n",
        "            self.model.classifier.high_classifier = nn.Conv2d(128, output_shape, kernel_size=1, stride=1)\n",
        "            self.model.classifier.low_classifier = nn.Conv2d(40, output_shape, kernel_size=1, stride=1)\n",
        "            try:\n",
        "                self.model.aux_classifier[-1] = nn.Conv2d(256, output_shape, kernel_size=1, stride=1)\n",
        "                self.model.aux_classifier.add_module('softmax', nn.Softmax(dim=1))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        #Create optimiser and learning rate scheduler\n",
        "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        if self.model_name == 'DeepLabV3' or self.model_name == 'FCN':\n",
        "            if self.optimiser_type == 'SGD':\n",
        "                self.optimiser = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
        "            if self.optimiser_type == 'Adam': \n",
        "                self.optimiser = torch.optim.Adam(params, lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n",
        "            if self.optimiser_type == 'RMSprop':\n",
        "                self.optimiser = torch.optim.RMSprop(params, lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0.001, momentum=0.9)\n",
        "            self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=4, gamma=0.01) \n",
        "\n",
        "        if self.model_name == 'LRASPP':\n",
        "            if self.optimiser_type == 'SGD':\n",
        "                self.optimiser = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "            if self.optimiser_type == 'Adam': \n",
        "                self.optimiser = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n",
        "            if self.optimiser_type == 'RMSprop':\n",
        "                self.optimiser = torch.optim.RMSprop(params, lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0.001, momentum=0.9)\n",
        "            self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=3, gamma=0.01)\n",
        "\n",
        "        # Initialise either the weighted Cross Entropy Loss or unweighted\n",
        "        if self.loss_type == 'Standard_CEL':\n",
        "            self.loss = nn.CrossEntropyLoss()\n",
        "        # if self.loss_type == 'Weighted_CEL':\n",
        "            # self.loss = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        \n",
        "        #Optionally print out the new model architecture\n",
        "        if verbose:\n",
        "            print(summary(model=self.model, \n",
        "                input_size=(10, 3, 520, 520),\n",
        "                col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "                col_width=20,\n",
        "                row_settings=[\"var_names\"]\n",
        "            )) \n",
        "\n",
        "        return self.model\n",
        "\n",
        "m1 = Model('DeepLabV3', 'Adam', 'Standard_CEL', verbose=False)\n",
        "m2 = Model('FCN', 'Adam', 'Standard_CEL', verbose=False)\n",
        "m3 = Model('LRASPP', 'Adam', 'Standard_CEL', verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Garbage collect and empty cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class TrainTest():\n",
        "  def __init__(self, model_class, trainLoader, testLoader, n_epochs=3, batch_size=3, k=10):\n",
        "    \n",
        "    # Initialise model specific attributes\n",
        "    self.model_class = model_class\n",
        "    self.model = self.model_class.model.to(device)\n",
        "    self.lr_scheduler = self.model_class.lr_scheduler\n",
        "    self.optimiser = self.model_class.optimiser\n",
        "    self.loss = self.model_class.loss\n",
        "    # self.iou_accuracy = self.model_class.iou_acc\n",
        "    # self.pixel_accuracy = self.model_class.pix_acc\n",
        "\n",
        "\n",
        "    # Initialise remaining attributes\n",
        "    self.trainLoader = trainLoader\n",
        "    self.testLoader = testLoader\n",
        "    self.n_epochs = n_epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.k = k\n",
        "\n",
        "    # Check to see whether the combination of batch size and dataloader size will result in a batch size of 1 during training,\n",
        "    # which will break the model. Change the batch size if so.\n",
        "    while len(self.trainLoader)*(self.k - 1)/self.k % self.batch_size == 1:\n",
        "        proceed = input(f\"Invalid batch size. Proceed with batch_size = {self.batch_size + 1}?  [Y/N]\\n\")\n",
        "        if proceed.lower() == 'y':\n",
        "            self.batch_size += 1\n",
        "            break\n",
        "        self.batch_size = input(\"Enter batch size: \")\n",
        "    \n",
        "  def train_epoch(self, dataloader):\n",
        "          self.model.train()\n",
        "        #   iou_acc = 0.0\n",
        "        #   pix_acc = 0.0\n",
        "          train_loss = 0.0\n",
        "\n",
        "          scaler = GradScaler()\n",
        "\n",
        "          for idx, (images, labels) in enumerate(dataloader): \n",
        "              images, labels = images.to(device).float(), labels.to(device)\n",
        "              images, labels = images.to(device).float(), labels.to(device)\n",
        "\n",
        "              # Calculate output predicitons\n",
        "              self.optimiser.zero_grad()\n",
        "              with autocast(dtype=torch.float16): # Use automatic mixed precision training to optimise memory usage\n",
        "                  outputs = self.model(images)['out']\n",
        "                \n",
        "                  # Convert labels to 32 channel one-hot encoding\n",
        "                #   labels = encode_segmap(labels)\n",
        "\n",
        "                  # Calculate loss \n",
        "                  loss = self.loss(outputs, labels.float())\n",
        "                    \n",
        "              # Perform scaled backward pass and update loss and accuracy\n",
        "              scaler.scale(loss).backward()\n",
        "              scaler.step(self.optimiser)\n",
        "              scaler.update()\n",
        "              train_loss += loss.item() * images.size(0)\n",
        "            #   iou_acc += self.iou_accuracy(outputs, labels).item() * images.size(0)\n",
        "            #   pix_acc += self.pixel_accuracy(outputs, labels).item() * images.size(0)\n",
        "\n",
        "          # Update learning rate scheduler\n",
        "          self.lr_scheduler.step()\n",
        "\n",
        "        #   return iou_acc, pix_acc, train_loss\n",
        "          return  train_loss\n",
        "      \n",
        "  def validation_epoch(self, dataloader):\n",
        "      \n",
        "      self.model.eval()\n",
        "      validation_iou_acc = 0.0\n",
        "      validation_pix_acc = 0.0\n",
        "      validation_loss = 0.0\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          for idx, (images, labels) in enumerate(dataloader): \n",
        "              images, labels = images.to(device).float(), labels.to(device)\n",
        "    \n",
        "              # Calculate output predicitons\n",
        "              outputs = self.model(images)['out']\n",
        "\n",
        "              # Convert labels to 32 channel one-hot encoding\n",
        "              labels = encode_segmap(labels)\n",
        "\n",
        "              # Calculate accuracy and loss\n",
        "              loss = self.loss(outputs, labels.float())\n",
        "              validation_loss += loss.item() * images.size(0)\n",
        "              validation_iou_acc += self.iou_accuracy(outputs, labels).item() * images.size(0)\n",
        "              validation_pix_acc += self.pixel_accuracy(outputs, labels).item() * images.size(0)\n",
        "\n",
        "      return validation_iou_acc, validation_pix_acc, validation_loss\n",
        "\n",
        "  def k_fold_train(self, shuffle=True, verbose=True):\n",
        "\n",
        "      kfold = KFold(n_splits=self.k, shuffle=shuffle)\n",
        "      cv_average_train_iou_accuracy = []\n",
        "      cv_average_train_pixel_accuracy = []\n",
        "      cv_average_validation_iou_accuracy = []\n",
        "      cv_average_validation_pixel_accuracy = []\n",
        "      cv_average_train_loss = []\n",
        "      cv_average_validation_loss = []\n",
        "      start_time = time.time()\n",
        "\n",
        "      for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(self.trainLoader.dataset)):\n",
        "          print(\"Current Fold:\", fold_idx + 1)\n",
        "\n",
        "          # Create data loaders for the training and validation subsets\n",
        "          train_loader = DataLoader(self.trainLoader, batch_size=self.batch_size, sampler=SubsetRandomSampler(train_idx))\n",
        "          val_loader = DataLoader(self.trainLoader, batch_size=self.batch_size, sampler=SubsetRandomSampler(val_idx))\n",
        "          \n",
        "          # Initialse lists to keep track of accuracy at each epoch\n",
        "          epoch_train_iou_accuracy = []\n",
        "          epoch_train_pixel_accuracy = []\n",
        "          epoch_validation_iou_accuracy = []\n",
        "          epoch_validation_pixel_accuracy = []\n",
        "          epoch_train_loss = []\n",
        "          epoch_validation_loss = []\n",
        "\n",
        "          for epoch in range(self.n_epochs):\n",
        "\n",
        "              train_cumulative_iou_acc, train_cumulative_pix_acc, train_loss = TrainTest.train_epoch(self, train_loader)\n",
        "              validation_cumulative_iou_acc, validation_cumulative_pix_acc, validation_loss = TrainTest.validation_epoch(self, val_loader)\n",
        "              \n",
        "              # Find the IoU and Pixel accuracy, and loss\n",
        "              train_iou_acc = train_cumulative_iou_acc / len(train_loader.sampler) * 100\n",
        "              train_pix_acc = train_cumulative_pix_acc / len(train_loader.sampler) * 100\n",
        "              validation_iou_acc = validation_cumulative_iou_acc / len(val_loader.sampler) * 100\n",
        "              validation_pix_acc = validation_cumulative_pix_acc / len(val_loader.sampler) * 100\n",
        "              train_loss = train_loss / len(train_loader.sampler)\n",
        "              validation_loss = validation_loss / len(val_loader.sampler)\n",
        "\n",
        "              if verbose==True:\n",
        "                  print(\"Epoch:{}/{} Training IoU Acc: {:.2f}%, Validation IoU Acc: {:.2f}%, Training Pixel Acc: {:.2f}%, Validation Pixel Acc: {:.2f}%, Training Loss: {:.3f}, Validation Loss: {:.3f}\".format(\n",
        "                            epoch + 1, self.n_epochs, train_iou_acc, validation_iou_acc, train_pix_acc, validation_pix_acc, train_loss, validation_loss))\n",
        "              \n",
        "              # Append epoch lists keeping track of accuracy and loss at each epoch \n",
        "              epoch_train_iou_accuracy.append(train_iou_acc)\n",
        "              epoch_train_pixel_accuracy.append(train_pix_acc)\n",
        "              epoch_validation_iou_accuracy.append(validation_iou_acc)\n",
        "              epoch_validation_pixel_accuracy.append(validation_pix_acc)\n",
        "              epoch_train_loss.append(train_loss)\n",
        "              epoch_validation_loss.append(validation_loss)\n",
        "          \n",
        "          # Append the CV average lists with the list containing the accuracy and loss for the epochs just gone\n",
        "          cv_average_train_iou_accuracy.append(epoch_train_iou_accuracy)\n",
        "          cv_average_train_pixel_accuracy.append(epoch_train_pixel_accuracy)\n",
        "          cv_average_validation_iou_accuracy.append(epoch_validation_iou_accuracy)\n",
        "          cv_average_validation_pixel_accuracy.append(epoch_validation_pixel_accuracy)\n",
        "          cv_average_train_loss.append(epoch_train_loss)\n",
        "          cv_average_validation_loss.append(epoch_validation_loss)\n",
        "\n",
        "      # Calculate and print the training time\n",
        "      train_time = time.time() - start_time\n",
        "      print(f'The time taken to train the network was {int(train_time // 60)} mins {train_time % 60 :.0f} seconds')\n",
        "\n",
        "      # Find the average accuracy and loss for each epoch (averaging across k folds)\n",
        "      cv_average_train_iou_accuracy = np.mean(cv_average_train_iou_accuracy, axis=0)\n",
        "      cv_average_train_pixel_accuracy = np.mean(cv_average_train_pixel_accuracy, axis=0)\n",
        "      cv_average_validation_iou_accuracy = np.mean(cv_average_validation_iou_accuracy, axis=0)\n",
        "      cv_average_validation_pixel_accuracy = np.mean(cv_average_validation_pixel_accuracy, axis=0)\n",
        "      cv_average_train_loss = np.mean(cv_average_train_loss, axis=0)\n",
        "      cv_average_validation_loss = np.mean(cv_average_validation_loss, axis=0)\n",
        "      \n",
        "      self.cv_average_train_iou_accuracy, self.cv_average_train_pixel_accuracy = cv_average_train_iou_accuracy, cv_average_train_pixel_accuracy\n",
        "      self.cv_average_validation_iou_accuracy, self.cv_average_validation_pixel_accuracy = cv_average_validation_iou_accuracy, cv_average_validation_pixel_accuracy\n",
        "      self.cv_average_train_loss, self.cv_average_validation_loss = cv_average_train_loss, cv_average_validation_loss\n",
        "\n",
        "      return cv_average_train_iou_accuracy, cv_average_train_pixel_accuracy, cv_average_validation_iou_accuracy, cv_average_validation_pixel_accuracy, cv_average_train_loss, cv_average_validation_loss\n",
        "\n",
        "  def test(self):\n",
        "\n",
        "      self.test_loader = DataLoader(self.testLoader, batch_size=self.batch_size, shuffle=True)\n",
        "      \n",
        "      test_iou_acc, test_pix_acc = 0.0, 0.0\n",
        "      test_loss = 0.0\n",
        "      self.model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for idx, (images, labels) in enumerate(self.test_loader):\n",
        "              if idx == len(self.testLoader) // self.batch_size:   \n",
        "                  images_copy, labels_copy = images.to(\"cpu\").clone(), labels.to(\"cpu\").clone()\n",
        "              images, labels = images.to(device).float(), labels.to(device)\n",
        "              \n",
        "              # Calculate output predicitons\n",
        "              outputs = self.model(images)['out']\n",
        "  \n",
        "              # Convert labels to 32 channel one-hot encoding\n",
        "              labels = encode_segmap(labels)\n",
        "  \n",
        "              # Calculate accuracy and loss\n",
        "              test_iou_acc += self.iou_accuracy(outputs, labels).item() * images.size(0)\n",
        "              test_pix_acc += self.pixel_accuracy(outputs, labels).item() * images.size(0)\n",
        "              loss = self.loss(outputs, labels.float())\n",
        "              test_loss += loss.item() * images.size(0)\n",
        "      \n",
        "      # Calculate and print the final testing accuracy and loss\n",
        "      test_iou_acc = test_iou_acc / len(self.testLoader) * 100\n",
        "      test_pix_acc = test_pix_acc / len(self.testLoader) * 100\n",
        "      test_loss = test_loss / len(self.testLoader)\n",
        "\n",
        "      print(f\"Final Test IoU Accuracy: {test_iou_acc:.2f}%, Final Test Pixel Accuracy: {test_pix_acc:.2f}%, Final Test Loss: {test_loss:.3f}\")\n",
        "      \n",
        "      # Select random image from last batch and display predicted mask alongside original mask and image\n",
        "      idx = random.randint(0, images.shape[0])\n",
        "      original_image = np.transpose(images_copy[idx], (1, 2, 0))\n",
        "      true_mask = np.transpose(labels_copy[idx], (1, 2, 0))\n",
        "      outputs_copy = self.model(images)['out'].cpu()\n",
        "      outputs_copy = outputs_copy.detach()\n",
        "      predicted_mask = one_hot_to_rgb(outputs_copy[idx])\n",
        "      \n",
        "      plt.figure(figsize=(6,6))\n",
        "      \n",
        "      ax = plt.subplot2grid((2,4),(0,0), colspan=2)\n",
        "      ax.imshow(original_image)\n",
        "      plt.title(\"Original Image\")\n",
        "      plt.axis(\"off\")\n",
        "  \n",
        "      # Convert a one-hot image to RGB and display\n",
        "      ax1 = plt.subplot2grid((2,4),(0,2), colspan=2)\n",
        "      ax1.imshow(true_mask)\n",
        "      plt.title(\"True Mask\")\n",
        "      plt.axis(\"off\")\n",
        "  \n",
        "      ax2 = plt.subplot2grid((2,4),(1,1), colspan=2)\n",
        "      ax2.imshow(predicted_mask)\n",
        "      plt.title(\"Predicted Mask\")\n",
        "      plt.axis(\"off\")\n",
        "  \n",
        "      plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n",
        "      plt.show()\n",
        "  \n",
        "  def plot(self):\n",
        "\n",
        "      # Plot graph showing the IoU and Pixel accuracy for the training and validation sets across n epochs\n",
        "      x = [str(i+1) for i in range(self.n_epochs)]\n",
        "      plt.figure()\n",
        "      plt.title(f\"Cross-Validated Average Training Accuracy over {self.n_epochs} epochs\")\n",
        "      plt.plot(x, self.cv_average_train_iou_accuracy, label='Training IoU Accuracy')\n",
        "      plt.plot(x, self.cv_average_train_pixel_accuracy, label='Training Pixel Accuracy')\n",
        "      plt.plot(x, self.cv_average_validation_iou_accuracy, label='Validation IoU Accuracy')\n",
        "      plt.plot(x, self.cv_average_validation_pixel_accuracy, label='Validation Pixel Accuracy')\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Accuracy (%)\")\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "      # Plot graph showing the training and validation loss over n epochs\n",
        "      plt.figure()\n",
        "      plt.title(f\"Cross-Validated Average Training Loss over {self.n_epochs} epochs\")\n",
        "      plt.plot(x, self.cv_average_train_loss, color='red', label='Training Loss')\n",
        "      plt.plot(x, self.cv_average_validation_loss, color='blue', label='Validation Loss')\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend()        \n",
        "      plt.show()\n",
        "\n",
        "# run = TrainTest(deepLabV3Model, trainLoader, testLoader, n_epochs=2, batch_size=8, k=3)\n",
        "# run.k_fold_train()\n",
        "# run.test()\n",
        "# run.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rhyscooper/opt/anaconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "to",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/jg/m6l2g86n6vl9c_nbqcftb2hh0000gn/T/ipykernel_769/627074562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/jg/m6l2g86n6vl9c_nbqcftb2hh0000gn/T/ipykernel_769/1899770813.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m               \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m               \u001b[0;31m# Calculate output predicitons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image categories\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_animated\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplural\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: to"
          ]
        }
      ],
      "source": [
        "TT = TrainTest(m1, train_loader, test_loader, n_epochs=2, batch_size=8, k=3)\n",
        "TT.train_epoch(dataloader=train_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
